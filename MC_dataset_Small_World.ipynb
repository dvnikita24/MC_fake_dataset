{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c38764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12971469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>n_tweets</th>\n",
       "      <th>n_retweets</th>\n",
       "      <th>n_replies</th>\n",
       "      <th>n_users</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>retweet_ids</th>\n",
       "      <th>reply_ids</th>\n",
       "      <th>user_ids</th>\n",
       "      <th>retweet_relations</th>\n",
       "      <th>reply_relations</th>\n",
       "      <th>data_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claim-JuneFC18</td>\n",
       "      <td>WHO said artemisia plant cures Covid-19.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>«Il n’y a rien de nouveau, juste un effet gros...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>1258265925658259458</td>\n",
       "      <td>1258266492573044736,1258268929732104192,125828...</td>\n",
       "      <td>1258265928321699840,1258266668821872640,125826...</td>\n",
       "      <td>1096021910972977152,529538368,1258273607278624...</td>\n",
       "      <td>1258266492573044736-1258265925658259458-722618...</td>\n",
       "      <td>1258265928321699840-1258265925658259458-284967...</td>\n",
       "      <td>FakeCovidClaimFiltered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claim-JuneFC41</td>\n",
       "      <td>“There was no real scientific basis for belie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More Info, Fox News host Laura Ingraham speaks...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5148</td>\n",
       "      <td>3759</td>\n",
       "      <td>1258329173141094401,1258514570945032192,128065...</td>\n",
       "      <td>1280680704116998144</td>\n",
       "      <td>1280654816352624640,1280655159643660288,128065...</td>\n",
       "      <td>795107478157586432,866860504203046914,14261352...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1280654816352624640-1280654150955565056-133715...</td>\n",
       "      <td>FakeCovidClaimFiltered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claim-JuneFC46</td>\n",
       "      <td>Joe Biden has “written a letter of apology” fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More Info, President Donald Trump speaks durin...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "      <td>1257361723872419841,1257633359351746561,125770...</td>\n",
       "      <td>1257361909428486145,1257362106883719171,125736...</td>\n",
       "      <td>1257362357795336193,1257362452246859778,125736...</td>\n",
       "      <td>826295010542772224,1211394495654162433,1080666...</td>\n",
       "      <td>1257361909428486145-1257361723872419841-794136...</td>\n",
       "      <td>1257362357795336193-1257361723872419841-832319...</td>\n",
       "      <td>FakeCovidClaimFiltered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Claim-JuneFC47</td>\n",
       "      <td>Did US House Speaker Pelosi 'Hold Back' Corona...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hirsch, Laura and Kevin Breuninger.\\xa0“Trump ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1237873947925856256,1238121200989151232,123823...</td>\n",
       "      <td>1240854409371926528,1240858669895438336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1137466967269498880,18526305,72153358413834240...</td>\n",
       "      <td>1240854409371926528-1240661167724470273-113746...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FakeCovidClaimFiltered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Claim-JuneFC55</td>\n",
       "      <td>“A few hours ago, China officially announced ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tanto en Facebook como en Twitter se ha vuelto...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>1239254388377563143,1239378517697212422,124000...</td>\n",
       "      <td>1239258123950600192,1239379736222150662,123956...</td>\n",
       "      <td>1240026703860314112,1240060236838379521,124006...</td>\n",
       "      <td>1069892389425180672,828922917174071297,1079227...</td>\n",
       "      <td>1239258123950600192-1239254388377563143-633421...</td>\n",
       "      <td>1240026703860314112-1239378517697212422-664343...</td>\n",
       "      <td>FakeCovidClaimFiltered</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          news_id                                              title  url  \\\n",
       "0  Claim-JuneFC18           WHO said artemisia plant cures Covid-19.  NaN   \n",
       "1  Claim-JuneFC41   “There was no real scientific basis for belie...  NaN   \n",
       "2  Claim-JuneFC46  Joe Biden has “written a letter of apology” fo...  NaN   \n",
       "3  Claim-JuneFC47  Did US House Speaker Pelosi 'Hold Back' Corona...  NaN   \n",
       "4  Claim-JuneFC55   “A few hours ago, China officially announced ...  NaN   \n",
       "\n",
       "  publish_date source                                               text  \\\n",
       "0   2020-05-05    NaN  «Il n’y a rien de nouveau, juste un effet gros...   \n",
       "1   2020-05-04    NaN  More Info, Fox News host Laura Ingraham speaks...   \n",
       "2   2020-05-03    NaN  More Info, President Donald Trump speaks durin...   \n",
       "3   2020-03-11    NaN  Hirsch, Laura and Kevin Breuninger.\\xa0“Trump ...   \n",
       "4   2020-04-03    NaN  Tanto en Facebook como en Twitter se ha vuelto...   \n",
       "\n",
       "   labels  n_tweets  n_retweets  n_replies  n_users  \\\n",
       "0       1         1           8         43       43   \n",
       "1       1         4           1       5148     3759   \n",
       "2       1         3          57         12       66   \n",
       "3       1        10           2          0       12   \n",
       "4       1         5          15         13       23   \n",
       "\n",
       "                                           tweet_ids  \\\n",
       "0                                1258265925658259458   \n",
       "1  1258329173141094401,1258514570945032192,128065...   \n",
       "2  1257361723872419841,1257633359351746561,125770...   \n",
       "3  1237873947925856256,1238121200989151232,123823...   \n",
       "4  1239254388377563143,1239378517697212422,124000...   \n",
       "\n",
       "                                         retweet_ids  \\\n",
       "0  1258266492573044736,1258268929732104192,125828...   \n",
       "1                                1280680704116998144   \n",
       "2  1257361909428486145,1257362106883719171,125736...   \n",
       "3            1240854409371926528,1240858669895438336   \n",
       "4  1239258123950600192,1239379736222150662,123956...   \n",
       "\n",
       "                                           reply_ids  \\\n",
       "0  1258265928321699840,1258266668821872640,125826...   \n",
       "1  1280654816352624640,1280655159643660288,128065...   \n",
       "2  1257362357795336193,1257362452246859778,125736...   \n",
       "3                                                NaN   \n",
       "4  1240026703860314112,1240060236838379521,124006...   \n",
       "\n",
       "                                            user_ids  \\\n",
       "0  1096021910972977152,529538368,1258273607278624...   \n",
       "1  795107478157586432,866860504203046914,14261352...   \n",
       "2  826295010542772224,1211394495654162433,1080666...   \n",
       "3  1137466967269498880,18526305,72153358413834240...   \n",
       "4  1069892389425180672,828922917174071297,1079227...   \n",
       "\n",
       "                                   retweet_relations  \\\n",
       "0  1258266492573044736-1258265925658259458-722618...   \n",
       "1                                                NaN   \n",
       "2  1257361909428486145-1257361723872419841-794136...   \n",
       "3  1240854409371926528-1240661167724470273-113746...   \n",
       "4  1239258123950600192-1239254388377563143-633421...   \n",
       "\n",
       "                                     reply_relations               data_name  \n",
       "0  1258265928321699840-1258265925658259458-284967...  FakeCovidClaimFiltered  \n",
       "1  1280654816352624640-1280654150955565056-133715...  FakeCovidClaimFiltered  \n",
       "2  1257362357795336193-1257361723872419841-832319...  FakeCovidClaimFiltered  \n",
       "3                                                NaN  FakeCovidClaimFiltered  \n",
       "4  1240026703860314112-1239378517697212422-664343...  FakeCovidClaimFiltered  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#readin csv\n",
    "df= pd.read_csv(\"/Users/nikitadvortsevoy/Desktop/Foundations of CSS/Final project/Dataset/MC_Fake_dataset.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b3c57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28335, 18)\n",
      "\n",
      "Columns: ['news_id', 'title', 'url', 'publish_date', 'source', 'text', 'labels', 'n_tweets', 'n_retweets', 'n_replies', 'n_users', 'tweet_ids', 'retweet_ids', 'reply_ids', 'user_ids', 'retweet_relations', 'reply_relations', 'data_name']\n",
      "news_id              28335\n",
      "title                28326\n",
      "url                  21649\n",
      "publish_date         12072\n",
      "source                1352\n",
      "text                 27750\n",
      "labels                   2\n",
      "n_tweets               932\n",
      "n_retweets            1322\n",
      "n_replies              946\n",
      "n_users               1832\n",
      "tweet_ids            28008\n",
      "retweet_ids          24608\n",
      "reply_ids            18818\n",
      "user_ids             28049\n",
      "retweet_relations    24521\n",
      "reply_relations      18758\n",
      "data_name               11\n",
      "dtype: int64\n",
      "news_id                  0\n",
      "title                    0\n",
      "url                   6661\n",
      "publish_date         10439\n",
      "source               14393\n",
      "text                   202\n",
      "labels                   0\n",
      "n_tweets                 0\n",
      "n_retweets               0\n",
      "n_replies                0\n",
      "n_users                  0\n",
      "tweet_ids                7\n",
      "retweet_ids           3430\n",
      "reply_ids             9203\n",
      "user_ids                 0\n",
      "retweet_relations     3519\n",
      "reply_relations       9262\n",
      "data_name                0\n",
      "dtype: int64\n",
      "news_id               0.00\n",
      "title                 0.00\n",
      "url                  23.51\n",
      "publish_date         36.84\n",
      "source               50.80\n",
      "text                  0.71\n",
      "labels                0.00\n",
      "n_tweets              0.00\n",
      "n_retweets            0.00\n",
      "n_replies             0.00\n",
      "n_users               0.00\n",
      "tweet_ids             0.02\n",
      "retweet_ids          12.11\n",
      "reply_ids            32.48\n",
      "user_ids              0.00\n",
      "retweet_relations    12.42\n",
      "reply_relations      32.69\n",
      "data_name             0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#CHECKING THE DATA\n",
    "\n",
    "#number of rows and columns\n",
    "print(df.shape)\n",
    "\n",
    "#columns\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "#checking unique fields\n",
    "print(df.nunique(dropna=True).head(20))\n",
    "\n",
    "#checking for Nan's\n",
    "print(df.isna().sum())\n",
    "\n",
    "#checking for Nan's in % \n",
    "print((df.isna().mean() * 100).round(2))\n",
    "\n",
    "#data looks suitable for our purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f29625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['1258266492573044736', '1258265925658259458', '722618840371372037', '284967242']\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 ()\n",
    "# dividing Tweet IDs from user ID's according to -  retweet_relations: retweet relations indicated by a list of tokens {tweet_ID_A}-{tweet_ID_B}-{user_ID of tweet A}-{user_ID of tweet B} denoting A retweets\n",
    "\n",
    "# tacking one row\n",
    "ret_relations_str = df.loc[0, \"retweet_relations\"]\n",
    "\n",
    "#spliting tokens to get rid of the gaps\n",
    "ret_tokens = ret_relations_str.split(\",\")\n",
    "\n",
    "#getting rid of gaps\n",
    "ret_clean_tokens = []\n",
    "for t in ret_tokens:\n",
    "    ret_clean_tokens.append(t.strip())\n",
    "\n",
    "ret_tokens = ret_clean_tokens\n",
    "\n",
    "#splitting ID's and checking it on several rows \n",
    "\n",
    "ret_splitted = ret_tokens[0].split(\"-\")\n",
    "print(len(ret_splitted))\n",
    "print(ret_splitted)\n",
    "\n",
    "#nice, it's working\n",
    "# tried on one row to check if it works \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f13cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[('722618840371372037', '284967242'), ('2309085185', '284967242'), ('1062422785249722368', '284967242'), ('1258273607278624769', '284967242'), ('529538368', '284967242')]\n"
     ]
    }
   ],
   "source": [
    "#STEP 2\n",
    "#Creating edge list for retweets - who(user A) retweeted or replied to whom (user B)\n",
    "\n",
    "retweet_edges = []\n",
    "\n",
    "for ret_tok in ret_tokens:\n",
    "    parts = ret_tok.split(\"-\")\n",
    "    ret_user_a = parts[2]\n",
    "    ret_user_b = parts[3]\n",
    "    retweet_edges.append((ret_user_a, ret_user_b))\n",
    "\n",
    "#checking\n",
    "print(len(retweet_edges))\n",
    "print(retweet_edges[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d56af16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "[('284967242', '284967242'), ('932575358569930752', '284967242'), ('989212314204360705', '284967242'), ('1234160752757477377', '284967242'), ('1163015951417319425', '284967242')]\n"
     ]
    }
   ],
   "source": [
    "#STEP 2.1\n",
    "#Doing the same with repplies - structure is the same reply_relations: reply relations indicated by a list of tokens {tweet_ID_A}-{tweet_ID_B}-{user_ID of tweet A}-{user_ID of tweet B} denoting A replies B\n",
    "\n",
    "# tacking one row\n",
    "rep_relations_str = df.loc[0, \"reply_relations\"]\n",
    "\n",
    "#spliting tokens to get rid of the gaps\n",
    "rep_tokens = rep_relations_str.split(\",\")\n",
    "\n",
    "#getting rid of gaps\n",
    "rep_clean_tokens = []\n",
    "for t in rep_tokens:\n",
    "    rep_clean_tokens.append(t.strip())\n",
    "\n",
    "rep_tokens = rep_clean_tokens\n",
    "\n",
    "#splitting ID's and checking it on several rows \n",
    "\n",
    "rep_splitted = rep_tokens[0].split(\"-\")\n",
    "\n",
    "#creating edge list \n",
    "reply_edges = []\n",
    "\n",
    "for tok in rep_tokens:\n",
    "    parts = tok.split(\"-\")\n",
    "    rep_user_a = parts[2]\n",
    "    rep_user_b = parts[3]\n",
    "    reply_edges.append((rep_user_a, rep_user_b))\n",
    "\n",
    "\n",
    "#checking\n",
    "print(len(reply_edges))\n",
    "print(reply_edges[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba682a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2833074\n",
      "[('Claim-JuneFC18', '722618840371372037', '284967242', 'retweet'), ('Claim-JuneFC18', '2309085185', '284967242', 'retweet'), ('Claim-JuneFC18', '1062422785249722368', '284967242', 'retweet'), ('Claim-JuneFC18', '1258273607278624769', '284967242', 'retweet'), ('Claim-JuneFC18', '529538368', '284967242', 'retweet')]\n"
     ]
    }
   ],
   "source": [
    "#STEP 3\n",
    "# Building edge list for retweets for news_id\n",
    "\n",
    "all_retweet_edges = []\n",
    "\n",
    "# preparing retweets relations\n",
    "for index in range(len(df)) :  \n",
    "    news_id = df.loc[index, \"news_id\"]\n",
    "    ret_relations_str = df.loc[index, \"retweet_relations\"]\n",
    "    if pd.isna(ret_relations_str):  # to avoid mistake with Nan\n",
    "        continue\n",
    "\n",
    "    ret_tokens = ret_relations_str.split(\",\")\n",
    "\n",
    "    ret_clean_tokens = []\n",
    "    for t in ret_tokens:\n",
    "        ret_clean_tokens.append(t.strip())\n",
    "\n",
    "    ret_tokens = ret_clean_tokens\n",
    "\n",
    "    # now edges\n",
    "    for ret_token in ret_tokens:\n",
    "        parts = ret_token.split(\"-\")\n",
    "        ret_user_a = parts[2]\n",
    "        ret_user_b = parts[3]\n",
    "        all_retweet_edges.append((news_id, ret_user_a, ret_user_b, \"retweet\"))\n",
    "\n",
    "#check\n",
    "print(len(all_retweet_edges))\n",
    "print(all_retweet_edges[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d629d007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1629038\n",
      "[('Claim-JuneFC18', '284967242', '284967242', 'reply'), ('Claim-JuneFC18', '932575358569930752', '284967242', 'reply'), ('Claim-JuneFC18', '989212314204360705', '284967242', 'reply'), ('Claim-JuneFC18', '1234160752757477377', '284967242', 'reply'), ('Claim-JuneFC18', '1163015951417319425', '284967242', 'reply')]\n"
     ]
    }
   ],
   "source": [
    "# STEP 3.2\n",
    "# Building edge list for replies for news_id\n",
    "\n",
    "all_reply_edges = []\n",
    "\n",
    "for index in range(len(df)):\n",
    "    news_id = df.loc[index, \"news_id\"]\n",
    "\n",
    "    rep_relations_str = df.loc[index, \"reply_relations\"]\n",
    "    if pd.isna(rep_relations_str):\n",
    "        continue\n",
    "\n",
    "    rep_tokens = rep_relations_str.split(\",\")\n",
    "\n",
    "    rep_clean_tokens = []\n",
    "    for t in rep_tokens:\n",
    "        rep_clean_tokens.append(t.strip())\n",
    "    rep_tokens = rep_clean_tokens\n",
    "\n",
    "    for rep_token in rep_tokens:\n",
    "        parts = rep_token.split(\"-\")\n",
    "        rep_user_a = parts[2]\n",
    "        rep_user_b = parts[3]\n",
    "        all_reply_edges.append((news_id, rep_user_a, rep_user_b, \"reply\"))\n",
    "\n",
    "\n",
    "\n",
    "#check\n",
    "print(len(all_reply_edges))\n",
    "print(all_reply_edges[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7fb0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_edges (retweet + reply) 4462112\n"
     ]
    }
   ],
   "source": [
    "#STEP 3.3\n",
    "# Unite to have all edges \n",
    "\n",
    "all_edges = all_retweet_edges + all_reply_edges\n",
    "\n",
    "print(\"All_edges (retweet + reply)\", len(all_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ba1a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>source_user</th>\n",
       "      <th>destination_user</th>\n",
       "      <th>interaction_kind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claim-JuneFC18</td>\n",
       "      <td>722618840371372037</td>\n",
       "      <td>284967242</td>\n",
       "      <td>retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claim-JuneFC18</td>\n",
       "      <td>2309085185</td>\n",
       "      <td>284967242</td>\n",
       "      <td>retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claim-JuneFC18</td>\n",
       "      <td>1062422785249722368</td>\n",
       "      <td>284967242</td>\n",
       "      <td>retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Claim-JuneFC18</td>\n",
       "      <td>1258273607278624769</td>\n",
       "      <td>284967242</td>\n",
       "      <td>retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Claim-JuneFC18</td>\n",
       "      <td>529538368</td>\n",
       "      <td>284967242</td>\n",
       "      <td>retweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          news_id          source_user destination_user interaction_kind\n",
       "0  Claim-JuneFC18   722618840371372037        284967242          retweet\n",
       "1  Claim-JuneFC18           2309085185        284967242          retweet\n",
       "2  Claim-JuneFC18  1062422785249722368        284967242          retweet\n",
       "3  Claim-JuneFC18  1258273607278624769        284967242          retweet\n",
       "4  Claim-JuneFC18            529538368        284967242          retweet"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a df with edges so it be easier to work with it\n",
    "\n",
    "edges_df = pd.DataFrame(all_edges,columns=[\"news_id\", \"source_user\", \"destination_user\", \"interaction_kind\"])\n",
    "\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24adef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing one event to test \n",
    "sample = df.loc[0,\"news_id\"]\n",
    "event_edges = edges_df[edges_df[\"news_id\"] == sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37a52557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes: 43\n",
      "edges: 47\n"
     ]
    }
   ],
   "source": [
    "# building undirected interaction graph by Wattz\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "graph = nx.Graph()\n",
    "for _, row in event_edges.iterrows():\n",
    "    src_usr = row[\"source_user\"]\n",
    "    dst_usr = row[\"destination_user\"]\n",
    "    graph.add_edge(src_usr, dst_usr)\n",
    "\n",
    "#checking numbers\n",
    "print(\"nodes:\", graph.number_of_nodes())\n",
    "print(\"edges:\", graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366436ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C - average clustering for one event: 0.13962432915921288\n"
     ]
    }
   ],
   "source": [
    "#calculating clustering coefficient C \n",
    "\n",
    "C = nx.average_clustering(graph)\n",
    "print(\"C - average clustering for one event:\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a02e8f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average shortest path length: 2.036544850498339\n"
     ]
    }
   ],
   "source": [
    "#calculating avarage shortest path length \n",
    "\n",
    "# finding all connected components in indirect graph\n",
    "components = list(nx.connected_components(graph)) \n",
    "\n",
    "#LCC - largest connected component, max - choosing the largest component by nodes  \n",
    "lcc_nodes = max(components, key=len) \n",
    "\n",
    "#tacking only the LCC nodes and edges between them \n",
    "graph_for_lcc = graph.subgraph(lcc_nodes)\n",
    "\n",
    "#calculation L - ASPL\n",
    "L = nx.average_shortest_path_length(graph_for_lcc)\n",
    "\n",
    "print(\"Average shortest path length:\",L)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we computed and build a user interaction graph for one event (one news_id)\n",
    "# We have NODES(source users and destination users) and EDGES (built on retweets and replies)\n",
    "\n",
    "# And now we have two metrics from \"small world\":\n",
    "# C - clustering - how local the network is\n",
    "# L - (avarage shortest path length) - the avarage number of steps between user (but only for LCC of the graph)\n",
    "\n",
    "# Not sure if we need to use LCC, don't quiet understand this part from lit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ea37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28335\n",
      "['Claim-JuneFC18' 'Claim-JuneFC41' 'Claim-JuneFC46' 'Claim-JuneFC47'\n",
      " 'Claim-JuneFC55']\n"
     ]
    }
   ],
   "source": [
    "#building a list for all news_id\n",
    "\n",
    "all_news = df[\"news_id\"].unique()\n",
    "\n",
    "\n",
    "\n",
    "#checking numbers\n",
    "print(len(all_news))\n",
    "print(all_news[:5])\n",
    "\n",
    "#all rows included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0949057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed events: 22438\n",
      "first 5 results: [('Claim-JuneFC18', 0.13962432915921288, 2.036544850498339), ('Claim-JuneFC46', 0.03230769230769231, 1.9976621858562245), ('Claim-JuneFC47', 0.0, 1.3333333333333333), ('Claim-JuneFC55', 0.08902691511387163, 1.7777777777777777), ('Claim-JuneFC149', 0.0, 1.8666666666666667)]\n"
     ]
    }
   ],
   "source": [
    "#building graph for events and computing C and L for it\n",
    "\n",
    "results = []\n",
    "\n",
    "# group edges once for faster access (no full-table scan each loop) - ADDED TO EXPEDITE CALCULATION PROCESS\n",
    "grouped = edges_df.groupby(\"news_id\")\n",
    "\n",
    "# Limit for very large networks - - ADDED TO EXPEDITE CALCULATION PROCESS - need to be added in limitations\n",
    "MAX_LCC_N = 3000\n",
    "\n",
    "# ********* C BLOCK STARTS HERE *********\n",
    "\n",
    "for nid in all_news: # all dataset\n",
    "    #event_edges = edges_df[edges_df[\"news_id\"] == nid] #filter edges for one event, one news_id\n",
    "\n",
    "    try: #ADDED TO EXPEDITE CALCULATION PROCESS\n",
    "        event_edges = grouped.get_group(nid)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    graph = nx.Graph() #building undirected graph\n",
    "\n",
    "    #adding edges src_usr->dst_usr\n",
    "    for _, row in event_edges.iterrows():\n",
    "        src_usr = row[\"source_user\"]\n",
    "        dst_usr = row[\"destination_user\"]\n",
    "        graph.add_edge(src_usr, dst_usr)\n",
    "\n",
    "    #exluding small graphs with small number of nodes and ages - we can change numbers and see how the results would change\n",
    "    if graph.number_of_nodes() < 3 or graph.number_of_edges() < 2:\n",
    "        continue\n",
    "\n",
    "    #results for C\n",
    "    C = nx.average_clustering(graph)\n",
    "\n",
    "    # ********* L BLOCK STARTS HERE *********\n",
    "    components = list(nx.connected_components(graph))\n",
    "    lcc_nodes = max(components, key=len)\n",
    "    graph_lcc = graph.subgraph(lcc_nodes).copy()\n",
    "\n",
    "    #exluding small graphs whith smal number of nodes - we can change numbers and see how the results would change\n",
    "    if graph_lcc.number_of_nodes() < 3:\n",
    "        continue\n",
    "\n",
    "    # CHANGED/ADDED: use MAX_LCC_N before calculating L (saves a lot of time)\n",
    "    N = graph_lcc.number_of_nodes()\n",
    "    if N > MAX_LCC_N:\n",
    "        continue\n",
    "\n",
    "    #results for L\n",
    "    L = nx.average_shortest_path_length(graph_lcc)\n",
    "\n",
    "    #saving results\n",
    "    results.append((nid, C, L))\n",
    "\n",
    "\n",
    "print(\"computed events:\", len(results))\n",
    "print(\"first 5 results:\", results[:5])\n",
    "\n",
    "\n",
    "# ********** CONCLUSION **********\n",
    "# This block computes the two core small-world diagnostics for each event (news_id) by building an undirected user interaction graph from retweet/reply edges.\n",
    "# It calculates the clustering coefficient C on the full event graph and the average shortest path length L on the event’s largest connected component (LCC) to ensure paths are defined. \n",
    "# The output is a list of per-event results (news_id, C, L) ready for later comparison and null-model normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f46b10ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed events: 22438\n",
      "first 5 rows: [('Claim-JuneFC18', 0.13962432915921288, 2.036544850498339, 0.029745293466223694, 4.072758848243067), ('Claim-JuneFC46', 0.03230769230769231, 1.9976621858562245, 0.02065778853914447, 4.649718102354574), ('Claim-JuneFC47', 0.0, 1.3333333333333333, 0.0, 1.3333333333333328), ('Claim-JuneFC55', 0.08902691511387163, 1.7777777777777777, 0.1135185185185185, 2.2190079365079365), ('Claim-JuneFC149', 0.0, 1.8666666666666667, 0.15972222222222218, 1.7916666666666667)]\n"
     ]
    }
   ],
   "source": [
    "#b building a random/null graph and calculating C_random and L_random for each news_id\n",
    "\n",
    "#generating R random graphs per event\n",
    "\n",
    "R = 20 # number of random graphs for event, we can change this number in order to see results for more random graphs\n",
    "null_results = []\n",
    "\n",
    "grouped = edges_df.groupby(\"news_id\") # added grouop by news_id to expedite the process!\n",
    "\n",
    "# Limit for very large networks - - ADDED TO EXPEDITE CALCULATION PROCESS - need to be added in limitations\n",
    "MAX_LCC_N = 3000  # CHANGED/ADDED: define once and actually use it below\n",
    "\n",
    "# ********* C BLOCK STARTS HERE *********\n",
    "\n",
    "for nid in all_news: #if number is here then we limiting our events \n",
    "\n",
    "    try:\n",
    "        event_edges = grouped.get_group(nid)  # GROUPBY: get edges for this news_id without scanning the whole table\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    # event_edges = edges_df[edges_df[\"news_id\"] == nid]\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    for _, row in event_edges.iterrows():\n",
    "        src_usr = row[\"source_user\"]\n",
    "        dst_usr = row[\"destination_user\"]\n",
    "        graph.add_edge(src_usr, dst_usr)\n",
    "\n",
    "    #exluding small graphs with small number of nodes and ages - we can change numbers and see how the results would change\n",
    "    if graph.number_of_nodes() < 3 or graph.number_of_edges() < 2:  # CHANGED: moved outside the edge-adding loop\n",
    "        continue  # CHANGED: now correctly skips the whole event (nid)\n",
    "\n",
    "    #results for C\n",
    "    C = nx.average_clustering(graph)  # CHANGED: moved outside the edge-adding loop\n",
    "\n",
    "    # ********* L BLOCK STARTS HERE *********\n",
    "\n",
    "    components = list(nx.connected_components(graph))  # CHANGED: moved outside the edge-adding loop\n",
    "    lcc_nodes = max(components, key=len)               # CHANGED: moved outside the edge-adding loop\n",
    "    graph_lcc = graph.subgraph(lcc_nodes).copy()       # CHANGED: moved outside the edge-adding loop\n",
    "\n",
    "    if graph_lcc.number_of_nodes() < 3:  # CHANGED: moved outside the edge-adding loop\n",
    "        continue\n",
    "\n",
    "    #matching the null model to the LCC size\n",
    "    N = graph_lcc.number_of_nodes()  # CHANGED: moved before L to allow early skipping\n",
    "    E = graph_lcc.number_of_edges()  # CHANGED: moved before L to allow early skipping\n",
    "\n",
    "    if N > MAX_LCC_N:  # CHANGED: use the limit before calculating L (saves time)\n",
    "        continue #exluding too big events to save time on calculating \n",
    "\n",
    "    #results for L\n",
    "    L = nx.average_shortest_path_length(graph_lcc)  # CHANGED: moved AFTER the size check\n",
    "\n",
    "    # building random/null graphs and calculating C_random and L_random \n",
    "\n",
    "    sum_C_rand = 0.0\n",
    "    sum_L_rand = 0.0\n",
    "    count_L = 0  # some random graphs might be too disconnected for L\n",
    "\n",
    "    for _ in range(R):\n",
    "        rand_g = nx.gnm_random_graph(N, E)  # undirected random graph with same N and E\n",
    "        sum_C_rand += nx.average_clustering(rand_g)\n",
    "\n",
    "        rand_components = list(nx.connected_components(rand_g))\n",
    "        rand_lcc_nodes = max(rand_components, key=len)\n",
    "        rand_lcc = rand_g.subgraph(rand_lcc_nodes).copy()\n",
    "\n",
    "        #exluding small graphs whith smal number of nodes - we can change numbers and see how the results would change\n",
    "        if rand_lcc.number_of_nodes() < 3:\n",
    "            continue\n",
    "\n",
    "        sum_L_rand += nx.average_shortest_path_length(rand_lcc)\n",
    "        #how many succesfull calculations for L for random graph\n",
    "        count_L += 1\n",
    "\n",
    "    # in case if all random graphs were too small for L then just skip this event\n",
    "    if count_L == 0:\n",
    "        continue\n",
    "\n",
    "    C_random = sum_C_rand / R\n",
    "    L_random = sum_L_rand / count_L # average only over valid random L calculations \n",
    "\n",
    "    #saving results\n",
    "    null_results.append((nid, C, L, C_random, L_random))\n",
    "\n",
    "print(\"processed events:\", len(null_results))  # CHANGED: print the correct list\n",
    "print(\"first 5 rows:\", null_results[:5])       # CHANGED: print the correct list\n",
    "\n",
    "\n",
    "# ********** CONCLUSION **********\n",
    "# In this block, we calculate small-world metrics for each news event (news_id) using the user interaction graph.\n",
    "# We calculate clustering (C) on the full event graph and average shortest path length (L) on the largest connected component (LCC).\n",
    "# To judge these values properly, we also build R random/null graphs with the same number of nodes (N) and edges (E) as the event’s LCC.\n",
    "# For these random graphs, we calculate C_random and L_random and take the average to get a baseline.\n",
    "# This gives us (C, L) and (C_random, L_random) for each event, so later we can normalize the results and compare Fake vs Real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64972a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           news_id  labels   C_ratio   L_ratio     sigma\n",
      "0   Claim-JuneFC18       1  4.693997  0.500041  9.387232\n",
      "1   Claim-JuneFC46       1  1.563947  0.429631  3.640212\n",
      "2   Claim-JuneFC47       1       NaN  1.000000       NaN\n",
      "3   Claim-JuneFC55       1  0.784250  0.801159  0.978895\n",
      "4  Claim-JuneFC149       1  0.000000  1.041860  0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1) Convert null_results into a DataFrame\n",
    "null_df = pd.DataFrame(\n",
    "    null_results,\n",
    "    columns=[\"news_id\", \"C\", \"L\", \"C_random\", \"L_random\"]\n",
    ")\n",
    "\n",
    "\n",
    "# print(\"null_df shape:\", null_df.shape)\n",
    "# print(null_df.head())\n",
    "\n",
    "# 2) Build a lookup table for labels (one label per news_id)\n",
    "labels_df = df[[\"news_id\", \"labels\"]].drop_duplicates()\n",
    "\n",
    "# print(\"labels_df shape:\", labels_df.shape)\n",
    "\n",
    "# 3) Merge labels into null_df\n",
    "final_df = null_df.merge(labels_df, on=\"news_id\", how=\"left\")\n",
    "\n",
    "# print(\"final_df shape:\", final_df.shape)\n",
    "# print(\"missing labels:\", final_df[\"labels\"].isna().sum())\n",
    "# print(final_df.head())\n",
    "\n",
    "# 4) Calculate normalized ratios and sigma\n",
    "final_df[\"C_ratio\"] = final_df[\"C\"] / final_df[\"C_random\"]\n",
    "final_df[\"L_ratio\"] = final_df[\"L\"] / final_df[\"L_random\"]\n",
    "final_df[\"sigma\"] = final_df[\"C_ratio\"] / final_df[\"L_ratio\"]\n",
    "\n",
    "print(final_df[[\"news_id\", \"labels\", \"C_ratio\", \"L_ratio\", \"sigma\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: master_small_world_table.csv\n",
      "Shape: (22438, 9)\n",
      "           news_id  labels (veracity: 0/1)  C (clustering)  \\\n",
      "0   Claim-JuneFC18                       1        0.139624   \n",
      "1   Claim-JuneFC46                       1        0.032308   \n",
      "2   Claim-JuneFC47                       1        0.000000   \n",
      "3   Claim-JuneFC55                       1        0.089027   \n",
      "4  Claim-JuneFC149                       1        0.000000   \n",
      "\n",
      "   L (avg shortest path on LCC)  C_random (clustering, null)  \\\n",
      "0                      2.036545                     0.029745   \n",
      "1                      1.997662                     0.020658   \n",
      "2                      1.333333                     0.000000   \n",
      "3                      1.777778                     0.113519   \n",
      "4                      1.866667                     0.159722   \n",
      "\n",
      "   L_random (avg shortest path on LCC, null)  C_ratio (C / C_random)  \\\n",
      "0                                   4.072759                4.693997   \n",
      "1                                   4.649718                1.563947   \n",
      "2                                   1.333333                     NaN   \n",
      "3                                   2.219008                0.784250   \n",
      "4                                   1.791667                0.000000   \n",
      "\n",
      "   L_ratio (L / L_random)  sigma ((C/C_random) / (L/L_random))  \n",
      "0                0.500041                             9.387232  \n",
      "1                0.429631                             3.640212  \n",
      "2                1.000000                                  NaN  \n",
      "3                0.801159                             0.978895  \n",
      "4                1.041860                             0.000000  \n"
     ]
    }
   ],
   "source": [
    "# MASTER TABLE (1 row = 1 news_id) for the whole team\n",
    "# Exports a CSV with renamed columns (short explanations in brackets)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1) Select columns and rename them with short explanations\n",
    "master_df = final_df[\n",
    "    [\"news_id\", \"labels\",\n",
    "     \"C\", \"L\", \"C_random\", \"L_random\",\n",
    "     \"C_ratio\", \"L_ratio\", \"sigma\"]\n",
    "].copy()\n",
    "\n",
    "master_df = master_df.rename(columns={\n",
    "    \"news_id\": \"news_id\",\n",
    "    \"labels\": \"labels (veracity: 0/1)\",\n",
    "    \"C\": \"C (clustering)\",\n",
    "    \"L\": \"L (avg shortest path on LCC)\",\n",
    "    \"C_random\": \"C_random (clustering, null)\",\n",
    "    \"L_random\": \"L_random (avg shortest path on LCC, null)\",\n",
    "    \"C_ratio\": \"C_ratio (C / C_random)\",\n",
    "    \"L_ratio\": \"L_ratio (L / L_random)\",\n",
    "    \"sigma\": \"sigma ((C/C_random) / (L/L_random))\"\n",
    "})\n",
    "\n",
    "# 2) Replace inf values with NaN to keep the CSV clean\n",
    "master_df = master_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 3) Save to CSV \n",
    "output_file = \"master_small_world_table.csv\"\n",
    "master_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54cc7f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: summary_small_world_table.csv\n",
      "Shape: (2, 11)\n",
      "   label (veracity: 0/1)  count  median_sigma  q25_sigma  q75_sigma  \\\n",
      "0                      0  17909           0.0        0.0        0.0   \n",
      "1                      1   2873           0.0        0.0        0.0   \n",
      "\n",
      "   median_C_ratio  q25_C_ratio  q75_C_ratio  median_L_ratio  q25_L_ratio  \\\n",
      "0             0.0          0.0          0.0        0.613536     0.445371   \n",
      "1             0.0          0.0          0.0        0.687274     0.473445   \n",
      "\n",
      "   q75_L_ratio  \n",
      "0     0.844730  \n",
      "1     0.909091  \n"
     ]
    }
   ],
   "source": [
    "# SUMMARY TABLE (Fake/Real)\n",
    "# Exports medians and quartiles for sigma, C_ratio, L_ratio by label\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use the same table you saved as master_df (or build it again from final_df)\n",
    "# master_df should contain: labels (veracity: 0/1), C_ratio, L_ratio, sigma\n",
    "\n",
    "# 1) Clean values: remove rows with NaN/inf in key metrics\n",
    "clean_master = master_df.copy()\n",
    "clean_master = clean_master.replace([np.inf, -np.inf], np.nan)\n",
    "clean_master = clean_master.dropna(subset=[\n",
    "    \"C_ratio (C / C_random)\",\n",
    "    \"L_ratio (L / L_random)\",\n",
    "    \"sigma ((C/C_random) / (L/L_random))\",\n",
    "    \"labels (veracity: 0/1)\"\n",
    "])\n",
    "\n",
    "# 2) Helper function to calculate q25, median, q75\n",
    "def q25(x): return x.quantile(0.25)\n",
    "def q75(x): return x.quantile(0.75)\n",
    "\n",
    "# 3) Group by label and calculate summary stats\n",
    "summary_df = clean_master.groupby(\"labels (veracity: 0/1)\").agg(\n",
    "    count=(\"sigma ((C/C_random) / (L/L_random))\", \"count\"),\n",
    "\n",
    "    median_sigma=(\"sigma ((C/C_random) / (L/L_random))\", \"median\"),\n",
    "    q25_sigma=(\"sigma ((C/C_random) / (L/L_random))\", q25),\n",
    "    q75_sigma=(\"sigma ((C/C_random) / (L/L_random))\", q75),\n",
    "\n",
    "    median_C_ratio=(\"C_ratio (C / C_random)\", \"median\"),\n",
    "    q25_C_ratio=(\"C_ratio (C / C_random)\", q25),\n",
    "    q75_C_ratio=(\"C_ratio (C / C_random)\", q75),\n",
    "\n",
    "    median_L_ratio=(\"L_ratio (L / L_random)\", \"median\"),\n",
    "    q25_L_ratio=(\"L_ratio (L / L_random)\", q25),\n",
    "    q75_L_ratio=(\"L_ratio (L / L_random)\", q75),\n",
    ").reset_index()\n",
    "\n",
    "# 4) Rename label column to a clearer name\n",
    "summary_df = summary_df.rename(columns={\n",
    "    \"labels (veracity: 0/1)\": \"label (veracity: 0/1)\"\n",
    "})\n",
    "\n",
    "# 5) Save to CSV\n",
    "output_file = \"summary_small_world_table.csv\"\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Saved:\", output_file)\n",
    "print(\"Shape:\", summary_df.shape)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01844e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in final_df: 22438\n",
      "\n",
      "Label distribution:\n",
      "labels\n",
      "0    19184\n",
      "1     3254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "C == 0 count: 18349 | share: 0.8178\n",
      "C_random == 0 count: 1656 | share: 0.0738\n",
      "\n",
      "Descriptive stats (overall):\n",
      "                  C      C_random             L      L_random\n",
      "count  22438.000000  22438.000000  22438.000000  22438.000000\n",
      "mean       0.005687      0.068194      1.997569      3.587285\n",
      "std        0.027265      0.083582      0.544872      2.009403\n",
      "min        0.000000      0.000000      1.000000      1.000000\n",
      "25%        0.000000      0.011575      1.714286      1.907708\n",
      "50%        0.000000      0.043928      1.913043      3.044042\n",
      "75%        0.000000      0.104630      2.032016      4.849674\n",
      "max        1.000000      1.000000      7.296797     10.581803\n",
      "\n",
      "Share of C == 0 by label:\n",
      "labels\n",
      "0    0.818442\n",
      "1    0.813768\n",
      "Name: C, dtype: float64\n",
      "\n",
      "Share of C_random == 0 by label:\n",
      "labels\n",
      "0    0.066462\n",
      "1    0.117087\n",
      "Name: C_random, dtype: float64\n",
      "\n",
      "NaN counts (ratios/sigma):\n",
      "C_ratio NaN: 1656\n",
      "L_ratio NaN: 0\n",
      "sigma NaN: 1656\n",
      "\n",
      "Descriptive stats for ratios (finite only):\n",
      "            C_ratio       L_ratio         sigma\n",
      "count  20782.000000  22438.000000  20782.000000\n",
      "mean       0.962924      0.676580      2.745995\n",
      "std        4.798478      0.244329     15.383659\n",
      "min        0.000000      0.190760      0.000000\n",
      "25%        0.000000      0.460439      0.000000\n",
      "50%        0.000000      0.656758      0.000000\n",
      "75%        0.000000      0.910816      0.000000\n",
      "max      164.082440      1.365639    618.898861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use final_df (the table with C, L, C_random, L_random, labels, and ratios if you calculated them)\n",
    "df_check = final_df.copy()\n",
    "\n",
    "print(\"Rows in final_df:\", len(df_check))\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df_check[\"labels\"].value_counts(dropna=False))\n",
    "\n",
    "# 1) How many zeros in C and C_random\n",
    "c_zero = (df_check[\"C\"] == 0).sum()\n",
    "cr_zero = (df_check[\"C_random\"] == 0).sum()\n",
    "\n",
    "print(\"\\nC == 0 count:\", c_zero, \"| share:\", round(c_zero / len(df_check), 4))\n",
    "print(\"C_random == 0 count:\", cr_zero, \"| share:\", round(cr_zero / len(df_check), 4))\n",
    "\n",
    "# 2) Basic descriptive stats\n",
    "print(\"\\nDescriptive stats (overall):\")\n",
    "print(df_check[[\"C\", \"C_random\", \"L\", \"L_random\"]].describe())\n",
    "\n",
    "# 3) Check zeros by label\n",
    "print(\"\\nShare of C == 0 by label:\")\n",
    "print(df_check.groupby(\"labels\")[\"C\"].apply(lambda s: (s == 0).mean()))\n",
    "\n",
    "print(\"\\nShare of C_random == 0 by label:\")\n",
    "print(df_check.groupby(\"labels\")[\"C_random\"].apply(lambda s: (s == 0).mean()))\n",
    "\n",
    "# 4) If ratios/sigma exist, check NaN/inf and stats\n",
    "if \"C_ratio\" in df_check.columns:\n",
    "    tmp = df_check.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    print(\"\\nNaN counts (ratios/sigma):\")\n",
    "    for col in [\"C_ratio\", \"L_ratio\", \"sigma\"]:\n",
    "        if col in tmp.columns:\n",
    "            print(col, \"NaN:\", tmp[col].isna().sum())\n",
    "\n",
    "    print(\"\\nDescriptive stats for ratios (finite only):\")\n",
    "    cols = [c for c in [\"C_ratio\", \"L_ratio\", \"sigma\"] if c in tmp.columns]\n",
    "    print(tmp[cols].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d91a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows used: 22438\n",
      "\n",
      "Counts by label:\n",
      "labels\n",
      "0    19184\n",
      "1     3254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "C (clustering) by label:\n",
      "        count      mean  median  min  max\n",
      "labels                                   \n",
      "0       19184  0.005380     0.0  0.0  1.0\n",
      "1        3254  0.007501     0.0  0.0  1.0\n",
      "\n",
      "L (avg shortest path on LCC) by label:\n",
      "        count      mean    median  min       max\n",
      "labels                                          \n",
      "0       19184  2.011596  1.916667  1.0  7.296797\n",
      "1        3254  1.914873  1.866667  1.0  7.161180\n"
     ]
    }
   ],
   "source": [
    "# Compare C and L between labels (0/1) using simple summary stats\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tmp = final_df.copy()\n",
    "tmp = tmp.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Keep only rows where C and L exist\n",
    "tmp = tmp.dropna(subset=[\"labels\", \"C\", \"L\"])\n",
    "\n",
    "print(\"Rows used:\", len(tmp))\n",
    "print(\"\\nCounts by label:\")\n",
    "print(tmp[\"labels\"].value_counts())\n",
    "\n",
    "print(\"\\nC (clustering) by label:\")\n",
    "print(tmp.groupby(\"labels\")[\"C\"].agg([\"count\", \"mean\", \"median\", \"min\", \"max\"]))\n",
    "\n",
    "print(\"\\nL (avg shortest path on LCC) by label:\")\n",
    "print(tmp.groupby(\"labels\")[\"L\"].agg([\"count\", \"mean\", \"median\", \"min\", \"max\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "80374531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "METRIC: L_ratio\n",
      "n_real: 19184 | n_fake: 3254\n",
      "Median Real: 0.6433944339973632\n",
      "Median Fake: 0.7525643123223118\n",
      "Observed diff (Fake - Real): 0.10916987832494862\n",
      "Permutation test p-value (two-sided): 0.0\n",
      "\n",
      "==============================\n",
      "METRIC: L\n",
      "n_real: 19184 | n_fake: 3254\n",
      "Median Real: 1.9166666666666667\n",
      "Median Fake: 1.8666666666666667\n",
      "Observed diff (Fake - Real): -0.050000000000000044\n",
      "Permutation test p-value (two-sided): 0.0\n"
     ]
    }
   ],
   "source": [
    "#PERMUTATION TEST\n",
    "\n",
    "# SETTINGS\n",
    "P = 3000          # number of permutations (start 2000–5000)\n",
    "SEED = 42\n",
    "REAL = 0\n",
    "FAKE = 1\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def permutation_test_median_diff(x_fake, x_real, P, rng):\n",
    "    \"\"\"\n",
    "    Permutation test for diff in medians: median(fake) - median(real)\n",
    "    Keeps group sizes fixed.\n",
    "    \"\"\"\n",
    "    n_fake = len(x_fake)\n",
    "    combined = np.concatenate([x_fake, x_real])\n",
    "\n",
    "    obs = np.median(x_fake) - np.median(x_real)\n",
    "\n",
    "    diffs = []\n",
    "    for _ in range(P):\n",
    "        perm = rng.permutation(combined)\n",
    "        perm_fake = perm[:n_fake]\n",
    "        perm_real = perm[n_fake:]\n",
    "        diffs.append(np.median(perm_fake) - np.median(perm_real))\n",
    "\n",
    "    diffs = np.array(diffs)\n",
    "    p_value = np.mean(np.abs(diffs) >= np.abs(obs))  # two-sided p-value\n",
    "    return obs, p_value\n",
    "\n",
    "# Choose metrics to compare (recommend L_ratio first; sigma can be zero-heavy)\n",
    "metrics = [\"L_ratio\", \"L\"]  # you can add \"C_ratio\" or \"sigma\" if you want\n",
    "\n",
    "for metric in metrics:\n",
    "    # Prepare clean data for this metric\n",
    "    tmp = final_df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"labels\", metric])\n",
    "\n",
    "    x_real = tmp.loc[tmp[\"labels\"] == REAL, metric].values\n",
    "    x_fake = tmp.loc[tmp[\"labels\"] == FAKE, metric].values\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"METRIC:\", metric)\n",
    "    print(\"n_real:\", len(x_real), \"| n_fake:\", len(x_fake))\n",
    "\n",
    "    if len(x_real) == 0 or len(x_fake) == 0:\n",
    "        print(\"Skip: one group is empty after cleaning.\")\n",
    "        continue\n",
    "\n",
    "    med_real = np.median(x_real)\n",
    "    med_fake = np.median(x_fake)\n",
    "    obs_diff = med_fake - med_real\n",
    "\n",
    "    print(\"Median Real:\", med_real)\n",
    "    print(\"Median Fake:\", med_fake)\n",
    "    print(\"Observed diff (Fake - Real):\", obs_diff)\n",
    "\n",
    "    obs, p_val = permutation_test_median_diff(x_fake, x_real, P, rng)\n",
    "    print(\"Permutation test p-value (two-sided):\", p_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3ea5696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_real: 17909 | n_fake: 2873\n",
      "Share Real (C_ratio > 1): 0.1455692668490703\n",
      "Share Fake (C_ratio > 1): 0.15001740341106856\n",
      "Observed diff (Fake - Real): 0.004448136561998273\n",
      "Permutation p-value (two-sided): 0.531\n",
      "p-value resolution approx: 0.0002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Settings\n",
    "P = 5000\n",
    "SEED = 42\n",
    "REAL = 0\n",
    "FAKE = 1\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# 1) Clean data for C_ratio (keep only valid baseline cases)\n",
    "tmp = final_df.replace([np.inf, -np.inf], np.nan).copy()\n",
    "tmp[\"C_ratio\"] = tmp[\"C\"] / tmp[\"C_random\"]\n",
    "tmp = tmp.dropna(subset=[\"labels\", \"C_ratio\", \"C_random\"])\n",
    "tmp = tmp[tmp[\"C_random\"] > 0]\n",
    "\n",
    "x_real = tmp.loc[tmp[\"labels\"] == REAL, \"C_ratio\"].values\n",
    "x_fake = tmp.loc[tmp[\"labels\"] == FAKE, \"C_ratio\"].values\n",
    "\n",
    "print(\"n_real:\", len(x_real), \"| n_fake:\", len(x_fake))\n",
    "\n",
    "# 2) Observed difference in shares where C_ratio > 1\n",
    "share_real = np.mean(x_real > 1)\n",
    "share_fake = np.mean(x_fake > 1)\n",
    "obs_diff = share_fake - share_real\n",
    "\n",
    "print(\"Share Real (C_ratio > 1):\", share_real)\n",
    "print(\"Share Fake (C_ratio > 1):\", share_fake)\n",
    "print(\"Observed diff (Fake - Real):\", obs_diff)\n",
    "\n",
    "# 3) Permutation test for the share difference (two-sided)\n",
    "combined = np.concatenate([x_fake, x_real])\n",
    "n_fake = len(x_fake)\n",
    "\n",
    "perm_diffs = []\n",
    "for _ in range(P):\n",
    "    perm = rng.permutation(combined)\n",
    "    perm_fake = perm[:n_fake]\n",
    "    perm_real = perm[n_fake:]\n",
    "    perm_diffs.append(np.mean(perm_fake > 1) - np.mean(perm_real > 1))\n",
    "\n",
    "perm_diffs = np.array(perm_diffs)\n",
    "p_value = np.mean(np.abs(perm_diffs) >= np.abs(obs_diff))\n",
    "\n",
    "print(\"Permutation p-value (two-sided):\", p_value)\n",
    "print(\"p-value resolution approx:\", 1 / P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "723ee0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== L_ratio (L/L_random): median(Fake) - median(Real) ===\n",
      "n_real: 19184 | n_fake: 3254\n",
      "median Real: 0.6433944339973632\n",
      "median Fake: 0.7525643123223118\n",
      "diff (Fake-Real): 0.10916987832494862\n",
      "bootstrap 95% CI: (np.float64(0.08811908171464863), np.float64(0.13123227249914957))\n",
      "permutation p-value: 0.0 | resolution ~ 0.0002\n",
      "\n",
      "=== L (avg shortest path on LCC): median(Fake) - median(Real) ===\n",
      "n_real: 19184 | n_fake: 3254\n",
      "median Real: 1.9166666666666667\n",
      "median Fake: 1.8666666666666667\n",
      "diff (Fake-Real): -0.050000000000000044\n",
      "bootstrap 95% CI: (np.float64(-0.06593406593406592), np.float64(-0.04166666666666674))\n",
      "permutation p-value: 0.0 | resolution ~ 0.0002\n",
      "\n",
      "=== Share(C_ratio > 1): share(Fake) - share(Real) ===\n",
      "n_real: 17909 | n_fake: 2873\n",
      "share Real: 0.1455692668490703\n",
      "share Fake: 0.15001740341106856\n",
      "diff (Fake-Real): 0.004448136561998273\n",
      "bootstrap 95% CI: (np.float64(-0.008980330656841812), np.float64(0.01876591332477411))\n",
      "permutation p-value: 0.5266 | resolution ~ 0.0002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Settings\n",
    "# ----------------------------\n",
    "REAL = 0\n",
    "FAKE = 1\n",
    "SEED = 42\n",
    "B = 3000   # bootstrap iterations (can use 2000–5000)\n",
    "P = 5000   # permutation iterations\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: bootstrap CI for median difference\n",
    "# ----------------------------\n",
    "def bootstrap_ci_median_diff(x_fake, x_real, B, rng):\n",
    "    diffs = []\n",
    "    n_f = len(x_fake)\n",
    "    n_r = len(x_real)\n",
    "    for _ in range(B):\n",
    "        samp_f = rng.choice(x_fake, size=n_f, replace=True)\n",
    "        samp_r = rng.choice(x_real, size=n_r, replace=True)\n",
    "        diffs.append(np.median(samp_f) - np.median(samp_r))\n",
    "    diffs = np.array(diffs)\n",
    "    return np.percentile(diffs, 2.5), np.percentile(diffs, 97.5)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: permutation test for median difference\n",
    "# ----------------------------\n",
    "def perm_p_median_diff(x_fake, x_real, P, rng):\n",
    "    n_f = len(x_fake)\n",
    "    combined = np.concatenate([x_fake, x_real])\n",
    "    obs = np.median(x_fake) - np.median(x_real)\n",
    "    diffs = []\n",
    "    for _ in range(P):\n",
    "        perm = rng.permutation(combined)\n",
    "        diffs.append(np.median(perm[:n_f]) - np.median(perm[n_f:]))\n",
    "    diffs = np.array(diffs)\n",
    "    p = np.mean(np.abs(diffs) >= np.abs(obs))\n",
    "    return obs, p\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: bootstrap CI for share difference (share_fake - share_real)\n",
    "# ----------------------------\n",
    "def bootstrap_ci_share_diff(x_fake_bool, x_real_bool, B, rng):\n",
    "    diffs = []\n",
    "    n_f = len(x_fake_bool)\n",
    "    n_r = len(x_real_bool)\n",
    "    for _ in range(B):\n",
    "        samp_f = rng.choice(x_fake_bool, size=n_f, replace=True)\n",
    "        samp_r = rng.choice(x_real_bool, size=n_r, replace=True)\n",
    "        diffs.append(np.mean(samp_f) - np.mean(samp_r))\n",
    "    diffs = np.array(diffs)\n",
    "    return np.percentile(diffs, 2.5), np.percentile(diffs, 97.5)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: permutation test for share difference (share_fake - share_real)\n",
    "# ----------------------------\n",
    "def perm_p_share_diff(x_fake_bool, x_real_bool, P, rng):\n",
    "    n_f = len(x_fake_bool)\n",
    "    combined = np.concatenate([x_fake_bool, x_real_bool]).astype(int)\n",
    "    obs = np.mean(x_fake_bool) - np.mean(x_real_bool)\n",
    "    diffs = []\n",
    "    for _ in range(P):\n",
    "        perm = rng.permutation(combined)\n",
    "        diffs.append(np.mean(perm[:n_f]) - np.mean(perm[n_f:]))\n",
    "    diffs = np.array(diffs)\n",
    "    p = np.mean(np.abs(diffs) >= np.abs(obs))\n",
    "    return obs, p\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare clean data once\n",
    "# ----------------------------\n",
    "tmp = final_df.replace([np.inf, -np.inf], np.nan).copy()\n",
    "\n",
    "# Ensure ratios exist (if already exist, these overwrite consistently)\n",
    "tmp[\"C_ratio\"] = tmp[\"C\"] / tmp[\"C_random\"]\n",
    "tmp[\"L_ratio\"] = tmp[\"L\"] / tmp[\"L_random\"]\n",
    "tmp[\"sigma\"] = tmp[\"C_ratio\"] / tmp[\"L_ratio\"]\n",
    "\n",
    "# ----------------------------\n",
    "# A) L_ratio: median diff + CI + perm test\n",
    "# ----------------------------\n",
    "tmp_Lr = tmp.dropna(subset=[\"labels\", \"L_ratio\"])\n",
    "x_real = tmp_Lr.loc[tmp_Lr[\"labels\"] == REAL, \"L_ratio\"].values\n",
    "x_fake = tmp_Lr.loc[tmp_Lr[\"labels\"] == FAKE, \"L_ratio\"].values\n",
    "\n",
    "med_real = np.median(x_real)\n",
    "med_fake = np.median(x_fake)\n",
    "ci_lo, ci_hi = bootstrap_ci_median_diff(x_fake, x_real, B, rng)\n",
    "obs, pval = perm_p_median_diff(x_fake, x_real, P, rng)\n",
    "\n",
    "print(\"\\n=== L_ratio (L/L_random): median(Fake) - median(Real) ===\")\n",
    "print(\"n_real:\", len(x_real), \"| n_fake:\", len(x_fake))\n",
    "print(\"median Real:\", med_real)\n",
    "print(\"median Fake:\", med_fake)\n",
    "print(\"diff (Fake-Real):\", obs)\n",
    "print(\"bootstrap 95% CI:\", (ci_lo, ci_hi))\n",
    "print(\"permutation p-value:\", pval, \"| resolution ~\", 1/P)\n",
    "\n",
    "# SAVE results for plotting later (no re-calculation needed)\n",
    "effects = {}  # add once (if not already created)\n",
    "\n",
    "effects[\"L_ratio (Fake − Real)\"] = {\n",
    "    \"obs\": float(obs),\n",
    "    \"ci_low\": float(ci_lo),\n",
    "    \"ci_high\": float(ci_hi),\n",
    "    \"p\": float(pval)\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# B) L (on LCC): median diff + CI + perm test\n",
    "# ----------------------------\n",
    "tmp_L = tmp.dropna(subset=[\"labels\", \"L\"])\n",
    "x_real = tmp_L.loc[tmp_L[\"labels\"] == REAL, \"L\"].values\n",
    "x_fake = tmp_L.loc[tmp_L[\"labels\"] == FAKE, \"L\"].values\n",
    "\n",
    "med_real = np.median(x_real)\n",
    "med_fake = np.median(x_fake)\n",
    "ci_lo, ci_hi = bootstrap_ci_median_diff(x_fake, x_real, B, rng)\n",
    "obs, pval = perm_p_median_diff(x_fake, x_real, P, rng)\n",
    "\n",
    "print(\"\\n=== L (avg shortest path on LCC): median(Fake) - median(Real) ===\")\n",
    "print(\"n_real:\", len(x_real), \"| n_fake:\", len(x_fake))\n",
    "print(\"median Real:\", med_real)\n",
    "print(\"median Fake:\", med_fake)\n",
    "print(\"diff (Fake-Real):\", obs)\n",
    "print(\"bootstrap 95% CI:\", (ci_lo, ci_hi))\n",
    "print(\"permutation p-value:\", pval, \"| resolution ~\", 1/P)\n",
    "\n",
    "effects[\"L (Fake − Real)\"] = {\n",
    "    \"obs\": float(obs),\n",
    "    \"ci_low\": float(ci_lo),\n",
    "    \"ci_high\": float(ci_hi),\n",
    "    \"p\": float(pval)\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# C) Clustering signal as a SHARE: share(C_ratio > 1) diff + CI + perm test\n",
    "# (cleaning: require C_random > 0 so the ratio is defined)\n",
    "# ----------------------------\n",
    "tmp_Cr = tmp.dropna(subset=[\"labels\", \"C_ratio\", \"C_random\"])\n",
    "tmp_Cr = tmp_Cr[tmp_Cr[\"C_random\"] > 0]\n",
    "\n",
    "real_bool = (tmp_Cr.loc[tmp_Cr[\"labels\"] == REAL, \"C_ratio\"].values > 1)\n",
    "fake_bool = (tmp_Cr.loc[tmp_Cr[\"labels\"] == FAKE, \"C_ratio\"].values > 1)\n",
    "\n",
    "share_real = np.mean(real_bool)\n",
    "share_fake = np.mean(fake_bool)\n",
    "\n",
    "ci_lo, ci_hi = bootstrap_ci_share_diff(fake_bool, real_bool, B, rng)\n",
    "obs, pval = perm_p_share_diff(fake_bool, real_bool, P, rng)\n",
    "\n",
    "print(\"\\n=== Share(C_ratio > 1): share(Fake) - share(Real) ===\")\n",
    "print(\"n_real:\", len(real_bool), \"| n_fake:\", len(fake_bool))\n",
    "print(\"share Real:\", share_real)\n",
    "print(\"share Fake:\", share_fake)\n",
    "print(\"diff (Fake-Real):\", obs)\n",
    "print(\"bootstrap 95% CI:\", (ci_lo, ci_hi))\n",
    "print(\"permutation p-value:\", pval, \"| resolution ~\", 1/P)\n",
    "\n",
    "effects[\"Share(C_ratio > 1) (Fake − Real)\"] = {\n",
    "    \"obs\": float(obs),\n",
    "    \"ci_low\": float(ci_lo),\n",
    "    \"ci_high\": float(ci_hi),\n",
    "    \"p\": float(pval)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34bb13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
